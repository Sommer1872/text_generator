{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Punctuation\n",
    "We'll be splitting the script into a word array using spaces as delimiters.  However, punctuations like periods and exclamation marks make it hard for the neural network to distinguish between the word \"bye\" and \"bye!\".\n",
    "\n",
    "Here, we implement the function `token_lookup` to return a dict that will be used to tokenize symbols like \"!\" into \"||Exclamation_Mark||\".  We create a dictionary for the symbols, where the symbol is the key and value is the token.\n",
    "\n",
    "This dictionary will be used to token the symbols and add the delimiter (space) around it.  This separates the symbols as it's own word, making it easier for the neural network to predict on the next word. We don't use a token that could be confused as a word. Instead of using the token \"dash\", we use therfore use \"||dash||\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    Load Dataset from File\n",
    "    \"\"\"\n",
    "    input_file = os.path.join(path)\n",
    "    with open(input_file, \"r\") as f:\n",
    "        data = f.read()\n",
    "\n",
    "    return data\n",
    "\n",
    "def token_lookup():\n",
    "    \"\"\"\n",
    "    Generates a dict to turn punctuation into a token.\n",
    "    :return: Tokenize dictionary where the key is the punctuation and the value is the token\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    tokenize = {\n",
    "        '.': '||period||',\n",
    "        ',': '||comma||',\n",
    "        '\"': '||quotation_mark||',\n",
    "        ';': '||semicolon||',\n",
    "        '!': '||exclamation_mark||',\n",
    "        '?': '||question_mark||',\n",
    "        '(': '||left_parentheses||',\n",
    "        ')': '||right_parentheses||',\n",
    "        '--':'||dash||',\n",
    "        '\\n':'||return||'\n",
    "    }\n",
    "    return tokenize\n",
    "\n",
    "def preprocess_and_save_data(dataset_path, token_lookup, create_lookup_tables):\n",
    "    \"\"\"\n",
    "    Preprocesses Text Data\n",
    "    \"\"\"\n",
    "    text = load_data(dataset_path)\n",
    "    \n",
    "    # Ignore notice, since we don't use it for analysing the data\n",
    "    text = text[81:]\n",
    "\n",
    "    token_dict = token_lookup()\n",
    "    for key, token in token_dict.items():\n",
    "        text = text.replace(key, ' {} '.format(token))\n",
    "\n",
    "    text = text.lower()\n",
    "    text = text.split()\n",
    "\n",
    "    vocab_to_int, int_to_vocab = create_lookup_tables(text)\n",
    "    int_text = [vocab_to_int[word] for word in text]\n",
    "    pickle.dump((int_text, vocab_to_int, int_to_vocab, token_dict), open('preprocess.p', 'wb'))\n",
    "\n",
    "\n",
    "def load_preprocess():\n",
    "    \"\"\"\n",
    "    Loads the Preprocessed Training data and return them in batches of <batch_size> or less\n",
    "    \"\"\"\n",
    "    return pickle.load(open('preprocess.p', mode='rb'))\n",
    "\n",
    "\n",
    "def save_params(params):\n",
    "    \"\"\"\n",
    "    Saves parameters to file\n",
    "    \"\"\"\n",
    "    pickle.dump(params, open('params.p', 'wb'))\n",
    "\n",
    "\n",
    "def load_params():\n",
    "    \"\"\"\n",
    "    Loads parameters from file\n",
    "    \"\"\"\n",
    "    return pickle.load(open('params.p', mode='rb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocessing Functions\n",
    "\n",
    "\n",
    "### Lookup Table\n",
    "To create a word embedding, we first need to transform the words to ids.  In this function, we create two dictionaries:\n",
    "- Dictionary to go from the words to an id, we'll call `vocab_to_int`\n",
    "- Dictionary to go from the id to word, we'll call `int_to_vocab`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Creates lookup tables for vocabulary\n",
    "    :param text: The text split into words\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    vocab = set(text)\n",
    "    vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "    int_to_vocab = dict(enumerate(vocab))\n",
    "    return vocab_to_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = './data/anna.txt'\n",
    "text = load_data(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "We can play around with `view_sentence_range` to view different parts of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 29230\n",
      "\n",
      "The sentences 5000 to 5200:\n",
      "settled herself comfortably. An invalid lady had already lain down to\n",
      "sleep. Two other ladies began talking to Anna, and a stout elderly lady\n",
      "tucked up her feet, and made observations about the heating of the\n",
      "train. Anna answered a few words, but not foreseeing any entertainment\n",
      "from the conversation, she asked Annushka to get a lamp, hooked it onto\n",
      "the arm of her seat, and took from her bag a paper knife and an English\n",
      "novel. At first her reading made no progress. The fuss and bustle were\n",
      "disturbing; then when the train had started, she could not help\n",
      "listening to the noises; then the snow beating on the left window and\n",
      "sticking to the pane, and the sight of the muffled guard passing by,\n",
      "covered with snow on one side, and the conversations about the terrible\n",
      "snowstorm raging outside, distracted her attention. Farther on, it was\n",
      "continually the same again and again: the same shaking and rattling, the\n",
      "same snow on the window, the same rapid transitions from steaming heat\n",
      "to cold, and back again to heat, the same passing glimpses of the same\n",
      "figures in the twilight, and the same voices, and Anna began to read and\n",
      "to understand what she read. Annushka was already dozing, the red bag on\n",
      "her lap, clutched by her broad hands, in gloves, of which one was torn.\n",
      "Anna Arkadyevna read and understood, but it was distasteful to her to\n",
      "read, that is, to follow the reflection of other people's lives. She had\n",
      "too great a desire to live herself. If she read that the heroine of the\n",
      "novel was nursing a sick man, she longed to move with noiseless steps\n",
      "about the room of a sick man; if she read of a member of Parliament\n",
      "making a speech, she longed to be delivering the speech; if she read of\n",
      "how Lady Mary had ridden after the hounds, and had provoked her\n",
      "sister-in-law, and had surprised everyone by her boldness, she too\n",
      "wished to be doing the same. But there was no chance of doing anything;\n",
      "and twisting the smooth paper knife in her little hands, she forced\n",
      "herself to read.\n",
      "\n",
      "The hero of the novel was already almost reaching his English happiness,\n",
      "a baronetcy and an estate, and Anna was feeling a desire to go with him\n",
      "to the estate, when she suddenly felt that _he_ ought to feel ashamed,\n",
      "and that she was ashamed of the same thing. But what had he to be\n",
      "ashamed of? \"What have I to be ashamed of?\" she asked herself in injured\n",
      "surprise. She laid down the book and sank against the back of the chair,\n",
      "tightly gripping the paper cutter in both hands. There was nothing. She\n",
      "went over all her Moscow recollections. All were good, pleasant. She\n",
      "remembered the ball, remembered Vronsky and his face of slavish\n",
      "adoration, remembered all her conduct with him: there was nothing\n",
      "shameful. And for all that, at the same point in her memories, the\n",
      "feeling of shame was intensified, as though some inner voice, just at\n",
      "the point when she thought of Vronsky, were saying to her, \"Warm, very\n",
      "warm, hot.\" \"Well, what is it?\" she said to herself resolutely, shifting\n",
      "her seat in the lounge. \"What does it mean? Am I afraid to look it\n",
      "straight in the face? Why, what is it? Can it be that between me and\n",
      "this officer boy there exist, or can exist, any other relations than\n",
      "such as are common with every acquaintance?\" She laughed contemptuously\n",
      "and took up her book again; but now she was definitely unable to follow\n",
      "what she read. She passed the paper knife over the window pane, then\n",
      "laid its smooth, cool surface to her cheek, and almost laughed aloud at\n",
      "the feeling of delight that all at once without cause came over her. She\n",
      "felt as though her nerves were strings being strained tighter and\n",
      "tighter on some sort of screwing peg. She felt her eyes opening wider\n",
      "and wider, her fingers and toes twitching nervously, something within\n",
      "oppressing her breathing, while all shapes and sounds seemed in the\n",
      "uncertain half-light to strike her with unaccustomed vividness. Moments\n",
      "of doubt were continually coming upon her, when she was uncertain\n",
      "whether the train were going forwards or backwards, or were standing\n",
      "still altogether; whether it were Annushka at her side or a stranger.\n",
      "\"What's that on the arm of the chair, a fur cloak or some beast? And\n",
      "what am I myself? Myself or some other woman?\" She was afraid of giving\n",
      "way to this delirium. But something drew her towards it, and she could\n",
      "yield to it or resist it at will. She got up to rouse herself, and\n",
      "slipped off her plaid and the cape of her warm dress. For a moment she\n",
      "regained her self-possession, and realized that the thin peasant who had\n",
      "come in wearing a long overcoat, with buttons missing from it, was the\n",
      "stoveheater, that he was looking at the thermometer, that it was the\n",
      "wind and snow bursting in after him at the door; but then everything\n",
      "grew blurred again.... That peasant with the long waist seemed to be\n",
      "gnawing something on the wall, the old lady began stretching her legs\n",
      "the whole length of the carriage, and filling it with a black cloud;\n",
      "then there was a fearful shrieking and banging, as though someone were\n",
      "being torn to pieces; then there was a blinding dazzle of red fire\n",
      "before her eyes and a wall seemed to rise up and hide everything. Anna\n",
      "felt as though she were sinking down. But it was not terrible, but\n",
      "delightful. The voice of a man muffled up and covered with snow shouted\n",
      "something in her ear. She got up and pulled herself together; she\n",
      "realized that they had reached a station and that this was the guard.\n",
      "She asked Annushka to hand her the cape she had taken off and her shawl,\n",
      "put them on and moved towards the door.\n",
      "\n",
      "\"Do you wish to get out?\" asked Annushka.\n",
      "\n",
      "\"Yes, I want a little air. It's very hot in here.\" And she opened the\n",
      "door. The driving snow and the wind rushed to meet her and struggled\n",
      "with her over the door. But she enjoyed the struggle.\n",
      "\n",
      "She opened the door and went out. The wind seemed as though lying in\n",
      "wait for her; with gleeful whistle it tried to snatch her up and bear\n",
      "her off, but she clung to the cold door post, and holding her skirt got\n",
      "down onto the platform and under the shelter of the carriages. The wind\n",
      "had been powerful on the steps, but on the platform, under the lee of\n",
      "the carriages, there was a lull. With enjoyment she drew deep breaths of\n",
      "the frozen, snowy air, and standing near the carriage looked about the\n",
      "platform and the lighted station.\n",
      "\n",
      "\n",
      "\n",
      "Chapter 30\n",
      "\n",
      "\n",
      "The raging tempest rushed whistling between the wheels of the carriages,\n",
      "about the scaffolding, and round the corner of the station. The\n",
      "carriages, posts, people, everything that was to be seen was covered\n",
      "with snow on one side, and was getting more and more thickly covered.\n",
      "For a moment there would come a lull in the storm, but then it would\n",
      "swoop down again with such onslaughts that it seemed impossible to stand\n",
      "against it. Meanwhile men ran to and fro, talking merrily together,\n",
      "their steps crackling on the platform as they continually opened and\n",
      "closed the big doors. The bent shadow of a man glided by at her feet,\n",
      "and she heard sounds of a hammer upon iron. \"Hand over that telegram!\"\n",
      "came an angry voice out of the stormy darkness on the other side. \"This\n",
      "way! No. 28!\" several different voices shouted again, and muffled\n",
      "figures ran by covered with snow. Two gentlemen with lighted cigarettes\n",
      "passed by her. She drew one more deep breath of the fresh air, and had\n",
      "just put her hand out of her muff to take hold of the door post and get\n",
      "back into the carriage, when another man in a military overcoat, quite\n",
      "close beside her, stepped between her and the flickering light of the\n",
      "lamp post. She looked round, and the same instant recognized Vronsky's\n",
      "face. Putting his hand to the peak of his cap, he bowed to her and\n",
      "asked, Was there anything she wanted? Could he be of any service to her?\n",
      "She gazed rather a long while at him without answering, and, in spite of\n",
      "the shadow in which he was standing, she saw, or fancied she saw, both\n",
      "the expression of his face and his eyes. It was again that expression of\n",
      "reverential ecstasy which had so worked upon her the day before. More\n",
      "than once she had told herself during the past few days, and again only\n",
      "a few moments before, that Vronsky was for her only one of the hundreds\n",
      "of young men, forever exactly the same, that are met everywhere, that\n",
      "she would never allow herself to bestow a thought upon him. But now at\n",
      "the first instant of meeting him, she was seized by a feeling of joyful\n",
      "pride. She had no need to ask why he had come. She knew as certainly as\n",
      "if he had told her that he was here to be where she was.\n",
      "\n",
      "\"I didn't know you were going. What are you coming for?\" she said,\n",
      "letting fall the hand with which she had grasped the door post. And\n",
      "irrepressible delight and eagerness shone in her face.\n",
      "\n",
      "\"What am I coming for?\" he repeated, looking straight into her eyes.\n",
      "\"You know that I have come to be where you are,\" he said; \"I can't help\n",
      "it.\"\n",
      "\n",
      "At that moment the wind, as it were, surmounting all obstacles, sent the\n",
      "snow flying from the carriage roofs, and clanked some sheet of iron it\n",
      "had torn off, while the hoarse whistle of the engine roared in front,\n",
      "plaintively and gloomily. All the awfulness of the storm seemed to her\n",
      "more splendid now. He had said what her soul longed to hear, though she\n",
      "feared it with her reason. She made no answer, and in her face he saw\n",
      "conflict.\n",
      "\n",
      "\"Forgive me, if you dislike what I said,\" he said humbly.\n",
      "\n",
      "He had spoken courteously, deferentially, yet so firmly, so stubbornly,\n",
      "that for a long while she could make no answer.\n",
      "\n",
      "\"It's wrong, what you say, and I beg you, if you're a good man, to\n",
      "forget what you've said, as I forget it,\" she said at last.\n",
      "\n",
      "\"Not one word, not one gesture of yours shall I, could I, ever\n",
      "forget...\"\n",
      "\n",
      "\"Enough, enough!\" she cried trying assiduously to give a stern\n",
      "expression to her face, into which he was gazing greedily. And clutching\n",
      "at the cold door post, she clambered up the steps and got rapidly into\n",
      "the corridor of the carriage. But in the little corridor she paused,\n",
      "going over in her imagination what had happened. Though she could not\n",
      "recall her own words or his, she realized instinctively that the\n",
      "momentary conversation had brought them fearfully closer; and she was\n",
      "panic-stricken and blissful at it. After standing still a few seconds,\n",
      "she went into the carriage and sat down in her place. The overstrained\n",
      "condition which had tormented her before did not only come back, but was\n",
      "intensified, and reached such a pitch that she was afraid every minute\n",
      "that something would snap within her from the excessive tension. She did\n",
      "not sleep all night. But in that nervous tension, and in the visions\n",
      "that filled her imagination, there was nothing disagreeable or gloomy:\n",
      "on the contrary there was something blissful, glowing, and exhilarating.\n",
      "Towards morning Anna sank into a doze, sitting in her place, and when\n",
      "she waked it was daylight and the train was near Petersburg. At once\n",
      "thoughts of home, of husband and of son, and the details of that day and\n",
      "the following came upon her.\n",
      "\n",
      "At Petersburg, as soon as the train stopped and she got out, the first\n",
      "person that attracted her attention was her husband. \"Oh, mercy! why do\n",
      "his ears look like that?\" she thought, looking at his frigid and\n",
      "imposing figure, and especially the ears that struck her at the moment\n",
      "as propping up the brim of his round hat. Catching sight of her, he came\n",
      "to meet her, his lips falling into their habitual sarcastic smile, and\n",
      "his big, tired eyes looking straight at her. An unpleasant sensation\n",
      "gripped at her heart when she met his obstinate and weary glance, as\n",
      "though she had expected to see him different. She was especially struck\n",
      "by the feeling of dissatisfaction with herself that she experienced on\n",
      "meeting him. That feeling was an intimate, familiar feeling, like a\n",
      "consciousness of hypocrisy, which she experienced in her relations with\n",
      "her husband. But hitherto she had not taken note of the feeling, now she\n",
      "was clearly and painfully aware of it.\n",
      "\n",
      "\"Yes, as you see, your tender spouse, as devoted as the first year after\n",
      "marriage, burned with impatience to see you,\" he said in his deliberate,\n",
      "high-pitched voice, and in that tone which he almost always took with\n",
      "her, a tone of jeering at anyone who should say in earnest what he said.\n"
     ]
    }
   ],
   "source": [
    "view_sentence_range = (5000, 5200)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len({word: None for word in text.split()})))\n",
    "\n",
    "print()\n",
    "print('The sentences {} to {}:'.format(*view_sentence_range))\n",
    "print('\\n'.join(text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the data and save it to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "preprocess_and_save_data(data_dir, token_lookup, create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is the first checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "int_text, vocab_to_int, int_to_vocab, token_dict = pickle.load(open('preprocess.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "466523"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(int_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Check the Version of TensorFlow and Access to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.0.0\n",
      "Default GPU Device: /gpu:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_inputs():\n",
    "    \"\"\"\n",
    "    Creates TF Placeholders for input, targets, and learning rate.\n",
    "    :return: Tuple (input, targets, learning rate)\n",
    "    \"\"\"\n",
    "    inputs = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    \n",
    "    targets = tf.placeholder(tf.int32,[None, None], name='targets')\n",
    "    \n",
    "    learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    \n",
    "    return (inputs, targets, learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build RNN Cell and Initialize\n",
    "We stack one or more [`BasicLSTMCells`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell) in a [`MultiRNNCell`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell).\n",
    "\n",
    "- We initalize Cell State using the MultiRNNCell's [`zero_state()`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell#zero_state) function\n",
    "- We apply the name \"initial_state\" to the initial state using [`tf.identity()`](https://www.tensorflow.org/api_docs/python/tf/identity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_init_cell(batch_size, rnn_size):\n",
    "    \"\"\"\n",
    "    Create an RNN Cell and initialize it.\n",
    "    :param batch_size: Size of batches\n",
    "    :param rnn_size: Size of RNNs\n",
    "    :return: Tuple (cell, initialize state)\n",
    "    \"\"\"\n",
    "\n",
    "    num_layers = 2\n",
    "\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "\n",
    "    #drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    \n",
    "    cell = tf.contrib.rnn.MultiRNNCell([lstm])\n",
    "\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    initial_state = tf.identity(initial_state, name='initial_state')\n",
    "    \n",
    "    \n",
    "    return (cell, initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embed(input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Creates embedding for <input_data>.\n",
    "    :param input_data: TF placeholder for text input.\n",
    "    :param vocab_size: Number of words in vocabulary.\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Embedded input.\n",
    "    \"\"\"\n",
    "    \n",
    "    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n",
    "    \n",
    "    embed = tf.nn.embedding_lookup(embedding, input_data)\n",
    "    \n",
    "    return embed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build RNN\n",
    "We created a RNN Cell in the `get_init_cell()` function.  Time to use the cell to create a RNN.\n",
    "- Build the RNN using the [`tf.nn.dynamic_rnn()`](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn)\n",
    "- Apply the name \"final_state\" to the final state using [`tf.identity()`](https://www.tensorflow.org/api_docs/python/tf/identity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_rnn(cell, inputs):\n",
    "    \"\"\"\n",
    "    Create a RNN using a RNN Cell\n",
    "    :param cell: RNN Cell\n",
    "    :param inputs: Input text data\n",
    "    :return: Tuple (Outputs, Final State)\n",
    "    \"\"\"\n",
    "    \n",
    "    outputs, state = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)\n",
    "    \n",
    "    state = tf.identity(state, name='final_state')\n",
    "    \n",
    "    return (outputs, state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Neural Network\n",
    "We use the functions implemented above to:\n",
    "- Apply embedding to `input_data` using the `get_embed(input_data, vocab_size, embed_dim)` function.\n",
    "- Build RNN using `cell` and our `build_rnn(cell, inputs)` function.\n",
    "- Apply a fully connected layer with a linear activation and `vocab_size` as the number of outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_nn(cell, rnn_size, input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Build part of the neural network\n",
    "    :param cell: RNN cell\n",
    "    :param rnn_size: Size of rnns\n",
    "    :param input_data: Input data\n",
    "    :param vocab_size: Vocabulary size\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Tuple (Logits, FinalState)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "\n",
    "    embed = get_embed(input_data, vocab_size, embed_dim)\n",
    "\n",
    "    outputs, state = build_rnn(cell, embed)\n",
    "\n",
    "    logits = tf.contrib.layers.fully_connected(outputs, vocab_size, activation_fn=None)\n",
    "\n",
    "    return (logits, state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batches\n",
    "Implement `get_batches` to create batches of input and targets using `int_text`.  The batches should be a Numpy array with the shape `(number of batches, 2, batch size, sequence length)`. Each batch contains two elements:\n",
    "- The first element is a single batch of **input** with the shape `[batch size, sequence length]`\n",
    "- The second element is a single batch of **targets** with the shape `[batch size, sequence length]`\n",
    "\n",
    "(Notice that the last target value in the last batch is the first input value of the first batch. In this case, `1`. This is a common technique used when creating sequence batches, although it is rather unintuitive.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(int_text, batch_size, seq_length):\n",
    "    \"\"\"\n",
    "    Return batches of input and target\n",
    "    :param int_text: Text with the words replaced by their ids\n",
    "    :param batch_size: The size of batch\n",
    "    :param seq_length: The length of sequence\n",
    "    :return: A list where each item is a tuple of (batch of input, batch of target).\n",
    "    \"\"\"\n",
    "    n_batches = len(int_text) // (batch_size * seq_length)\n",
    "\n",
    "    # Drop the last few characters to make only full batches\n",
    "    xdata = np.array(int_text[: n_batches * batch_size * seq_length])\n",
    "    ydata = np.array(int_text[1: n_batches * batch_size * seq_length + 1])\n",
    "    \n",
    "    ydata[-1] = xdata[0]\n",
    "\n",
    "    x_batches = np.split(xdata.reshape(batch_size, -1), n_batches, 1)\n",
    "    y_batches = np.split(ydata.reshape(batch_size, -1), n_batches, 1)\n",
    "\n",
    "    return np.array(list(zip(x_batches, y_batches)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Training\n",
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "\n",
    "- Set `num_epochs` to the number of epochs.\n",
    "- Set `batch_size` to the batch size.\n",
    "- Set `rnn_size` to the size of the RNNs.\n",
    "- Set `embed_dim` to the size of the embedding.\n",
    "- Set `seq_length` to the length of sequence.\n",
    "- Set `learning_rate` to the learning rate.\n",
    "- Set `show_every_n_batches` to the number of batches the neural network should print progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "num_epochs = 200\n",
    "# Batch Size\n",
    "batch_size = 256\n",
    "# RNN Size\n",
    "rnn_size = 512\n",
    "# Embedding Dimension Size\n",
    "embed_dim = 256\n",
    "# Sequence Length\n",
    "seq_length = 15\n",
    "# Learning Rate\n",
    "learning_rate = 0.001\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 363\n",
    "\n",
    "save_dir = './save'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Graph\n",
    "Build the graph using the neural network we implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.contrib import seq2seq\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    vocab_size = len(int_to_vocab)\n",
    "    input_text, targets, lr = get_inputs()\n",
    "    input_data_shape = tf.shape(input_text)\n",
    "    cell, initial_state = get_init_cell(input_data_shape[0], rnn_size)\n",
    "    logits, final_state = build_nn(cell, rnn_size, input_text, vocab_size, embed_dim)\n",
    "\n",
    "    # Probabilities for generating words\n",
    "    probs = tf.nn.softmax(logits, name='probs')\n",
    "\n",
    "    # Loss function\n",
    "    cost = seq2seq.sequence_loss(logits,\n",
    "                                 targets,\n",
    "                                 tf.ones([input_data_shape[0], input_data_shape[1]]))\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "    # Gradient Clipping\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "Train the neural network on the preprocessed data.  If you have a hard time getting a good loss, check the [forms](https://discussions.udacity.com/) to see if anyone is having the same problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   0 Batch    0/121   train_loss = 9.571   Time elapsed: 0.0min\n",
      "\n",
      "Epoch   3 Batch    0/121   train_loss = 4.983   Time elapsed: 1.6min\n",
      "\n",
      "Epoch   6 Batch    0/121   train_loss = 4.508   Time elapsed: 3.2min\n",
      "\n",
      "Epoch   9 Batch    0/121   train_loss = 4.259   Time elapsed: 4.8min\n",
      "\n",
      "Epoch  12 Batch    0/121   train_loss = 4.061   Time elapsed: 6.4min\n",
      "\n",
      "Epoch  15 Batch    0/121   train_loss = 3.888   Time elapsed: 8.0min\n",
      "\n",
      "Epoch  18 Batch    0/121   train_loss = 3.724   Time elapsed: 9.6min\n",
      "\n",
      "Epoch  21 Batch    0/121   train_loss = 3.573   Time elapsed: 11.2min\n",
      "\n",
      "Epoch  24 Batch    0/121   train_loss = 3.436   Time elapsed: 12.8min\n",
      "\n",
      "Epoch  27 Batch    0/121   train_loss = 3.299   Time elapsed: 14.4min\n",
      "\n",
      "Epoch  30 Batch    0/121   train_loss = 3.174   Time elapsed: 16.0min\n",
      "\n",
      "Epoch  33 Batch    0/121   train_loss = 3.044   Time elapsed: 17.6min\n",
      "\n",
      "Epoch  36 Batch    0/121   train_loss = 2.922   Time elapsed: 19.2min\n",
      "\n",
      "Epoch  39 Batch    0/121   train_loss = 2.799   Time elapsed: 20.8min\n",
      "\n",
      "Epoch  42 Batch    0/121   train_loss = 2.688   Time elapsed: 22.4min\n",
      "\n",
      "Epoch  45 Batch    0/121   train_loss = 2.580   Time elapsed: 24.0min\n",
      "\n",
      "Epoch  48 Batch    0/121   train_loss = 2.470   Time elapsed: 25.6min\n",
      "\n",
      "Epoch  51 Batch    0/121   train_loss = 2.375   Time elapsed: 27.2min\n",
      "\n",
      "Epoch  54 Batch    0/121   train_loss = 2.295   Time elapsed: 28.8min\n",
      "\n",
      "Epoch  84 Batch    0/121   train_loss = 1.563   Time elapsed: 44.8min\n",
      "\n",
      "Epoch  87 Batch    0/121   train_loss = 1.503   Time elapsed: 46.4min\n",
      "\n",
      "Epoch  90 Batch    0/121   train_loss = 1.459   Time elapsed: 48.0min\n",
      "\n",
      "Epoch  93 Batch    0/121   train_loss = 1.401   Time elapsed: 49.6min\n",
      "\n",
      "Epoch  96 Batch    0/121   train_loss = 1.361   Time elapsed: 51.2min\n",
      "\n",
      "Epoch  99 Batch    0/121   train_loss = 1.304   Time elapsed: 52.8min\n",
      "\n",
      "Epoch 102 Batch    0/121   train_loss = 1.281   Time elapsed: 54.4min\n",
      "\n",
      "Epoch 117 Batch    0/121   train_loss = 1.059   Time elapsed: 62.4min\n",
      "\n",
      "Epoch 120 Batch    0/121   train_loss = 1.049   Time elapsed: 64.0min\n",
      "\n",
      "Epoch 123 Batch    0/121   train_loss = 0.969   Time elapsed: 65.6min\n",
      "\n",
      "Epoch 126 Batch    0/121   train_loss = 0.970   Time elapsed: 67.2min\n",
      "\n",
      "Epoch 135 Batch    0/121   train_loss = 0.887   Time elapsed: 72.0min\n",
      "\n",
      "Epoch 138 Batch    0/121   train_loss = 0.873   Time elapsed: 73.6min\n",
      "\n",
      "Epoch 141 Batch    0/121   train_loss = 0.803   Time elapsed: 75.2min\n",
      "\n",
      "Epoch 144 Batch    0/121   train_loss = 0.781   Time elapsed: 76.8min\n",
      "\n",
      "Epoch 147 Batch    0/121   train_loss = 0.755   Time elapsed: 78.4min\n",
      "\n",
      "Epoch 150 Batch    0/121   train_loss = 0.720   Time elapsed: 80.0min\n",
      "\n",
      "Epoch 153 Batch    0/121   train_loss = 0.695   Time elapsed: 81.6min\n",
      "\n",
      "Epoch 156 Batch    0/121   train_loss = 0.690   Time elapsed: 83.2min\n",
      "\n",
      "Epoch 159 Batch    0/121   train_loss = 0.669   Time elapsed: 84.8min\n",
      "\n",
      "Epoch 162 Batch    0/121   train_loss = 0.640   Time elapsed: 86.4min\n",
      "\n",
      "Epoch 165 Batch    0/121   train_loss = 0.659   Time elapsed: 88.0min\n",
      "\n",
      "Epoch 168 Batch    0/121   train_loss = 0.609   Time elapsed: 89.6min\n",
      "\n",
      "Epoch 171 Batch    0/121   train_loss = 0.583   Time elapsed: 91.2min\n",
      "\n",
      "Epoch 174 Batch    0/121   train_loss = 0.597   Time elapsed: 92.8min\n",
      "\n",
      "Epoch 177 Batch    0/121   train_loss = 0.563   Time elapsed: 94.4min\n",
      "\n",
      "Epoch 180 Batch    0/121   train_loss = 0.613   Time elapsed: 96.0min\n",
      "\n",
      "Epoch 183 Batch    0/121   train_loss = 0.566   Time elapsed: 97.6min\n",
      "\n",
      "Epoch 186 Batch    0/121   train_loss = 0.568   Time elapsed: 99.2min\n",
      "\n",
      "Epoch 189 Batch    0/121   train_loss = 0.584   Time elapsed: 100.8min\n",
      "\n",
      "Epoch 192 Batch    0/121   train_loss = 0.530   Time elapsed: 102.4min\n",
      "\n",
      "Epoch 195 Batch    0/121   train_loss = 0.532   Time elapsed: 104.0min\n",
      "\n",
      "Epoch 198 Batch    0/121   train_loss = 0.526   Time elapsed: 105.6min\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batches = get_batches(int_text, batch_size, seq_length)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(num_epochs):\n",
    "        state = sess.run(initial_state, {input_text: batches[0][0]})\n",
    "\n",
    "        for batch_i, (x, y) in enumerate(batches):\n",
    "            feed = {\n",
    "                input_text: x,\n",
    "                targets: y,\n",
    "                initial_state: state,\n",
    "                lr: learning_rate}\n",
    "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed)\n",
    "\n",
    "            # Show every <show_every_n_batches> batches\n",
    "            if (epoch_i * len(batches) + batch_i) % show_every_n_batches == 0:\n",
    "                elapsed_time = (time.time() - start_time) / 60\n",
    "                print(\"\".format())\n",
    "                print('Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}   Time elapsed: {:.1f}min'.format(\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    len(batches),\n",
    "                    train_loss,\n",
    "                    elapsed_time))\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_dir)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save parameters for checkpoint\n",
    "save_params((seq_length, save_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "_, vocab_to_int, int_to_vocab, token_dict = load_preprocess()\n",
    "seq_length, load_dir = load_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Generate Functions\n",
    "### Get Tensors\n",
    "We get the tensors from `loaded_graph` using the function [`get_tensor_by_name()`](https://www.tensorflow.org/api_docs/python/tf/Graph#get_tensor_by_name).  We use the following names:\n",
    "- \"input:0\"\n",
    "- \"initial_state:0\"\n",
    "- \"final_state:0\"\n",
    "- \"probs:0\"\n",
    "\n",
    "Return the tensors in the following tuple `(InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tensors(loaded_graph):\n",
    "    \"\"\"\n",
    "    Gets input, initial state, final state, and probabilities tensor from <loaded_graph>\n",
    "    :param loaded_graph: TensorFlow graph loaded from file\n",
    "    :return: Tuple (InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)\n",
    "    \"\"\"    \n",
    "    InputTensor = tf.Graph.get_tensor_by_name(loaded_graph, name='input:0')\n",
    "    InitialStateTensor = tf.Graph.get_tensor_by_name(loaded_graph, name='initial_state:0')\n",
    "    FinalStateTensor = tf.Graph.get_tensor_by_name(loaded_graph, name='final_state:0')\n",
    "    ProbsTensor = tf.Graph.get_tensor_by_name(loaded_graph, name='probs:0')\n",
    "    \n",
    "    return (InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose Word\n",
    "Implement the `pick_word()` function to select the next word using `probabilities`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_word(probabilities, int_to_vocab):\n",
    "    \"\"\"\n",
    "    Picks the next word in the generated text\n",
    "    :param probabilities: Probabilites of the next word\n",
    "    :param int_to_vocab: Dictionary of word ids as the keys and words as the values\n",
    "    :return: String of the predicted word\n",
    "    \"\"\"\n",
    "    return np.random.choice(list(int_to_vocab.values()), p=probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate new Text\n",
    "This will generate the new text. We can set `gen_length` to the length of text we want to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all about anna.\n",
      "\n",
      "sviazhsky questioned him too that day, she had scarcely had better\n",
      "quite free in moscow, as though she had been\n",
      "thinking of nothing but felt his whole figure and felt to blame. he had\n",
      "made an answer and pain, and for her part she had a great deal\n",
      "of her acquaintances, her lips was going out without leaving time to the sound\n",
      "opposite her, and began to move themselves, with her that\n",
      "sister-in-law was not to sign the baby, to her mother's coldness in his morning,\n",
      "and he felt with difficulty about this. the other, as an english put\n",
      "in her having balls, gone up in taking a cup of tea, with\n",
      "bare fingers, so that he would never allow the colonel.\n",
      "\n",
      "\" disagreeable division of women? be guided by its humiliation, and this is\n",
      "perhaps in the public domain print.\"\n",
      "\n",
      "though it was enough for kitty in his usual, and while he was in\n",
      "both the minutes above as\n"
     ]
    }
   ],
   "source": [
    "gen_length = 200\n",
    "\n",
    "prime_word = 'all'\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    input_text, initial_state, final_state, probs = get_tensors(loaded_graph)\n",
    "\n",
    "    # Sentences generation setup\n",
    "    gen_sentences = [prime_word]\n",
    "    prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
    "\n",
    "    # Generate sentences\n",
    "    for n in range(gen_length):\n",
    "        # Dynamic Input\n",
    "        dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
    "        dyn_seq_length = len(dyn_input[0])\n",
    "\n",
    "        # Get Prediction\n",
    "        probabilities, prev_state = sess.run(\n",
    "            [probs, final_state],\n",
    "            {input_text: dyn_input, initial_state: prev_state})\n",
    "        \n",
    "        pred_word = pick_word(probabilities[dyn_seq_length-1], int_to_vocab)\n",
    "\n",
    "        gen_sentences.append(pred_word)\n",
    "    \n",
    "    # Remove tokens\n",
    "    new_text = ' '.join(gen_sentences)\n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        new_text = new_text.replace(' ' + token.lower(), key)\n",
    "    new_text = new_text.replace('\\n ', '\\n')\n",
    "    new_text = new_text.replace('( ', '(')\n",
    "        \n",
    "    print(new_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
